= DIAAS Architecture

== DataStack

The core thing we have.

tied 1-1 with a git repo (and its production branch is production branch is production).

Pushing straight to master is allowed, but fast forward only (so you can't push if your local branch isn't up to date). In the future we should provide a more fine rained way to control this (so PR based work flow)

Don't forget: checking out the code locally should be a first class citizen. The webapp is an IDE, not "the" UI. We're assuming this is important to our cusotmers, time will tell.

A DataStack would be a project in gcloud, or an account in AWS.

== User

a human being, someone using the system. Given consultants and agnecies we'll want one person to be able to access multiple organisations or projects.

== Teams and Roles

We'll need these at some point to provide enough enterprise like permissions and access control. for now we can ignore them.

== WorkBench

Every user has (exactly) one WorkBench, and can pick which branch of which DataStack they want to work on at any one time (it is not possible for a user to be working on multiple branches/data stacks simultaneously.

A WorkBench is just a directoroy containing a bunch or git repos.

Note that a WorkBench is per user, not per session. So mulitple logins from differents devies will access the same set of files.

We could have made the WorkBench's state per-session, but making a logout (or brower crash) cause data lose seems too drastic, and a "recover lost sessoin" UI seems to complicated. this should keep things simple.

there are two important seletors in the UI, one for the current DataStack and one for the current branch in that stack.

== First Time Setup / Login

We don't have an explicit login/registratino step (for better or for wrose) so every time a new sessin is created we need to check "does this user exist" and if not setup an initial DataStack and WorkBench for them.

== Managing the IDE/GUI and direct code changes

so how do we allow both click-based UIs and code based "whatever"? YAML to the rescue!! Basically all the click based things, so basically all the extractors, are backed by a .yaml file in the /extractors/managed/ directory. in the /extractors/standard.py file we load these files (so one could technically disable this behaviour, though i'm not all sure why you'd want to), something like:

[source]
----
from diaas.extractors import YamlExtractor

YamlExtractor.load_from_dir(Path(__file__) / 'managed/')
----

one file in managed is one extractor, with a unique file name (i don't think we strictly need a unique name per extractor, but it feels like it'll make things easier, so we're going with it for now)

that is a comman approach for the UI vs code mix: The UI edits a yaml file, one .py file loads the yaml and builds an extracttor or transformation or whatever. other py files can be written by hand.


the repo layout is as follows:

- /extractors/
- /transforms/
- /visuals/ - currently empty

there's no code reason for the split into directories, it's just about
interal organisation. DIAAS itself will always load all the code in
order to generate airflow DAGs or run dbt transformations.

one thing to debate is if the central /diaas.py file configures this layout, in which case a user could change it, or not. if the user changes it it makes the UI much less useful, so why allow that?

we are running random code on our servers. we need something like a
server per workbench (maybe per git checkout is a better way to put
it). we will run the user's pyhton code, which could be malicous, on
our servers to collect and load and transform data. even if the code
isn't malicious we need some way to protect against infinte loops or
crazy resource usage. for now we are more than happy to ignore this
problem, but the moment we have 2 users on the same installation we
need to think about it. (one solution of course is to not have 2 users
on one installation and everyone gets their own infra (snowflake seems
to do this), not a bad idea, but probably a bit of over head). even
before we solve this we need a way for the backend to ask "what
extractions/transformations/modules/etc. live in thsi dir?" We should
put a `Dockerfile` in the root of the repo and this, single, image
will be used to run all the work bench's code (including airflow
itself). airflow's dependencies are weird, so at some point you want the airflow to be hidden and use dockeroperator to run the code, but that's not v0.

maybe we want to introduce the "organization" as a concept. each org
has one set of infra, it can have multiple work benches (though
generally i'd have only one) and user mgmt and permissions are al
per org. this would certainly make some things simpler.

we will have a minimal git tool in the UI. pull from master. commit (which is different from save). push. merge to master. reset hard. no rebase.
